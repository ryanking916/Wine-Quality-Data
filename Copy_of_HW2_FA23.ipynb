{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryanking916/Wine-Quality-Data/blob/main/Copy_of_HW2_FA23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y9GALMazqwO"
      },
      "source": [
        "# Homework 2 (Feed Forward Neural Networks)\n",
        "\n",
        "Choose a dataset that you're interested in from among these options (or choose your own data set as long as it's large enough and **you check with me** in advance):\n",
        "\n",
        "- [Boston Housing Data](https://github.com/selva86/datasets/blob/master/BostonHousing.csv) (More info [here](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html))\n",
        "- [Wine Quality Data](https://archive.ics.uci.edu/ml/datasets/wine+quality)\n",
        "- [Spam Emails](https://archive.ics.uci.edu/ml/datasets/Spambase)\n",
        "- [European Soccer Data](https://www.kaggle.com/datasets/hugomathien/soccer)(would only recommend if you want to spend some time joining and cleaning data)\n",
        "-\n",
        "\n",
        "Then Build a Deep FEED FORWARD Neural Network (No Convolutional or Recurrent Layers) using keras/tensorflow (at least 3 *hidden* layers) to predict either a category or a continuous value.\n",
        "\n",
        "Make sure that:\n",
        "\n",
        "- your NN has some sort of regularization (or multiple types if needed)\n",
        "- you've properly z-scored or otherwise scaled your data before training\n",
        "- your model architechture and loss function are appropriate for the problem\n",
        "- you print out at least 2 metrics for both train and test data to examine\n",
        "\n",
        "Then, using the SAME predictors and outcome, **build a simpler ML model from 392** (some options listed below with documentation) and check if your Neural Net did better (essentially I want you to PROVE whether you needed a neural network for the task or not).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2TZlnhxzqwT"
      },
      "source": [
        "\n",
        "- [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
        "- [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "- [KNN Regression](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
        "- [KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
        "- [Decision Tree Regression](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
        "- [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
        "- [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
        "- [Random Forest Regression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
        "\n",
        "\n",
        "Lastly, create a **technical report** discussing your model building process, the results, and your reflection on it. The report should follow the format in the example including an Introduction, Analysis, Methods, Results, and Reflection section. Your report is practice for presenting results to non-technical audiences in your Data Science career (e.g. your boss, CEO, shareholders...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFnOtJVNzqwT"
      },
      "source": [
        "# Technical Report Sections\n",
        "\n",
        "## Introduction\n",
        "An introduction should introduce the problem and data you're working on, give some background and relevant detail for the reader, and explain why it is important.\n",
        "\n",
        "## Analysis\n",
        "Any exploratory analysis of your data, and general summarization of the data (e.g. summary statistics, correlation heatmaps, graphs, information about the data...). Tell the reader about the types of variables you have and some general information about them, Plots and/or Tables are always great. This should also include any cleaning and joining you did.\n",
        "\n",
        "If you want a table you can make one with [this website](https://www.tablesgenerator.com/markdown_tables) and paste the markdown table here. For example:\n",
        "\n",
        "## Methods\n",
        "Explain the structure of your model and your approach to building it. This can also include changes you made to your model in the process of building it. Someone should be able to read your methods section and *generally* be able to tell exactly what architechture you used. However REMEMBER that this should be geared towards an audience who might not understand Tensorflow code.\n",
        "\n",
        "## Results\n",
        "Detailed discussion of how your model performed, and your discussion of how your model performed.\n",
        "\n",
        "## Reflection\n",
        "Reflections on what you learned/discovered in the process of doing the assignment. Write about any struggles you had (and hopefully overcame) during the process. Things you would do differently in the future, ways you'll approach similar problems in the future, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anDf1UU_zqwU"
      },
      "source": [
        "# What to Turn In\n",
        "\n",
        "- PDF of your technical report (rendered through Quarto)\n",
        "- your code as a .py, .ipynb, or link to github (you must turn it in either as a file, or a link to something that has timestamps of when the file was last edited)\n",
        "- a README file as a .txt or .md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdJPrKVV_1-y"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import tensorflow.keras as kb\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from plotnine import *\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score\n",
        "\n",
        "from sklearn.linear_model import LinearRegression # Linear Regression Model\n",
        "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXMOBKNTB-rV",
        "outputId": "103a7db3-dc82-4583-b1ab-5267461ca67a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
            "0            7.0              0.27         0.36            20.7      0.045   \n",
            "1            6.3              0.30         0.34             1.6      0.049   \n",
            "2            8.1              0.28         0.40             6.9      0.050   \n",
            "3            7.2              0.23         0.32             8.5      0.058   \n",
            "4            7.2              0.23         0.32             8.5      0.058   \n",
            "\n",
            "   free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n",
            "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
            "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
            "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
            "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
            "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
            "\n",
            "   alcohol  quality  \n",
            "0      8.8        6  \n",
            "1      9.5        6  \n",
            "2     10.1        6  \n",
            "3      9.9        6  \n",
            "4      9.9        6  \n"
          ]
        }
      ],
      "source": [
        "# Loading data using code from the Machine Learning UCI Repo\n",
        "\n",
        "# fetch dataset\n",
        "wine_quality = fetch_ucirepo(id=186)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = wine_quality.data.features\n",
        "y = wine_quality.data.targets\n",
        "\n",
        "# metadata\n",
        "#print(wine_quality.metadata)\n",
        "\n",
        "# variable information\n",
        "#print(wine_quality.variables)\n",
        "\n",
        "# Concatenating the features and targets\n",
        "wine = pd.concat([X, y], axis=1)\n",
        "\n",
        "# Displaying the first few rows\n",
        "print(wine.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing summary statistics\n",
        "print(wine.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATIbkraGY3jI",
        "outputId": "67338ad9-0a86-4c2f-9458-2c389da90275"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       fixed_acidity  volatile_acidity  citric_acid  residual_sugar  \\\n",
            "count    4898.000000       4898.000000  4898.000000     4898.000000   \n",
            "mean        6.854788          0.278241     0.334192        6.391415   \n",
            "std         0.843868          0.100795     0.121020        5.072058   \n",
            "min         3.800000          0.080000     0.000000        0.600000   \n",
            "25%         6.300000          0.210000     0.270000        1.700000   \n",
            "50%         6.800000          0.260000     0.320000        5.200000   \n",
            "75%         7.300000          0.320000     0.390000        9.900000   \n",
            "max        14.200000          1.100000     1.660000       65.800000   \n",
            "\n",
            "         chlorides  free_sulfur_dioxide  total_sulfur_dioxide      density  \\\n",
            "count  4898.000000          4898.000000           4898.000000  4898.000000   \n",
            "mean      0.045772            35.308085            138.360657     0.994027   \n",
            "std       0.021848            17.007137             42.498065     0.002991   \n",
            "min       0.009000             2.000000              9.000000     0.987110   \n",
            "25%       0.036000            23.000000            108.000000     0.991723   \n",
            "50%       0.043000            34.000000            134.000000     0.993740   \n",
            "75%       0.050000            46.000000            167.000000     0.996100   \n",
            "max       0.346000           289.000000            440.000000     1.038980   \n",
            "\n",
            "                pH    sulphates      alcohol      quality  \n",
            "count  4898.000000  4898.000000  4898.000000  4898.000000  \n",
            "mean      3.188267     0.489847    10.514267     5.877909  \n",
            "std       0.151001     0.114126     1.230621     0.885639  \n",
            "min       2.720000     0.220000     8.000000     3.000000  \n",
            "25%       3.090000     0.410000     9.500000     5.000000  \n",
            "50%       3.180000     0.470000    10.400000     6.000000  \n",
            "75%       3.280000     0.550000    11.400000     6.000000  \n",
            "max       3.820000     1.080000    14.200000     9.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ2Rr8x_Fq00",
        "outputId": "73cc113d-1dc0-4432-b645-beec819b249b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4898, 12)\n"
          ]
        }
      ],
      "source": [
        "# Setting predictor and predict variables\n",
        "predictors = [\"fixed_acidity\", \"volatile_acidity\", \"citric_acid\", \"residual_sugar\", \"chlorides\",\n",
        "              \"free_sulfur_dioxide\", \"total_sulfur_dioxide\", \"density\", \"pH\", \"sulphates\", \"alcohol\"]\n",
        "predict = \"quality\"\n",
        "\n",
        "print(wine.shape)\n",
        "X = wine[predictors]\n",
        "y = wine[predict]\n",
        "\n",
        "# Creating train and test splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\n",
        "\n",
        "# Z scoring predictors\n",
        "z = StandardScaler()\n",
        "X_train[predictors] = z.fit_transform(X_train[predictors])\n",
        "X_test[predictors] = z.transform(X_test[predictors])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk2veME7LPMl",
        "outputId": "eb449a85-8158-4c16-f4e5-d79e08e9af12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "123/123 [==============================] - 3s 12ms/step - loss: 1.9566 - accuracy: 0.3236 - mae: 0.1677 - val_loss: 1.4854 - val_accuracy: 0.4612 - val_mae: 0.1483\n",
            "Epoch 2/100\n",
            "123/123 [==============================] - 1s 7ms/step - loss: 1.4679 - accuracy: 0.4342 - mae: 0.1428 - val_loss: 1.3122 - val_accuracy: 0.4653 - val_mae: 0.1360\n",
            "Epoch 3/100\n",
            "123/123 [==============================] - 1s 7ms/step - loss: 1.3861 - accuracy: 0.4505 - mae: 0.1369 - val_loss: 1.2589 - val_accuracy: 0.4796 - val_mae: 0.1325\n",
            "Epoch 4/100\n",
            "123/123 [==============================] - 1s 5ms/step - loss: 1.3453 - accuracy: 0.4520 - mae: 0.1352 - val_loss: 1.2278 - val_accuracy: 0.4837 - val_mae: 0.1304\n",
            "Epoch 5/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.3132 - accuracy: 0.4653 - mae: 0.1333 - val_loss: 1.1965 - val_accuracy: 0.5092 - val_mae: 0.1284\n",
            "Epoch 6/100\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 1.2926 - accuracy: 0.4594 - mae: 0.1322 - val_loss: 1.1740 - val_accuracy: 0.5173 - val_mae: 0.1269\n",
            "Epoch 7/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.2728 - accuracy: 0.4684 - mae: 0.1306 - val_loss: 1.1629 - val_accuracy: 0.5102 - val_mae: 0.1266\n",
            "Epoch 8/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.2469 - accuracy: 0.4696 - mae: 0.1294 - val_loss: 1.1467 - val_accuracy: 0.5204 - val_mae: 0.1253\n",
            "Epoch 9/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.2397 - accuracy: 0.4834 - mae: 0.1285 - val_loss: 1.1283 - val_accuracy: 0.5582 - val_mae: 0.1240\n",
            "Epoch 10/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.2260 - accuracy: 0.4788 - mae: 0.1280 - val_loss: 1.1158 - val_accuracy: 0.5469 - val_mae: 0.1227\n",
            "Epoch 11/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.2066 - accuracy: 0.4855 - mae: 0.1264 - val_loss: 1.1048 - val_accuracy: 0.5622 - val_mae: 0.1221\n",
            "Epoch 12/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1950 - accuracy: 0.4926 - mae: 0.1256 - val_loss: 1.0988 - val_accuracy: 0.5582 - val_mae: 0.1217\n",
            "Epoch 13/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1977 - accuracy: 0.5015 - mae: 0.1257 - val_loss: 1.0825 - val_accuracy: 0.5500 - val_mae: 0.1199\n",
            "Epoch 14/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.1916 - accuracy: 0.4977 - mae: 0.1249 - val_loss: 1.0845 - val_accuracy: 0.5367 - val_mae: 0.1205\n",
            "Epoch 15/100\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 1.1705 - accuracy: 0.5056 - mae: 0.1241 - val_loss: 1.0711 - val_accuracy: 0.5500 - val_mae: 0.1190\n",
            "Epoch 16/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1703 - accuracy: 0.5158 - mae: 0.1234 - val_loss: 1.0670 - val_accuracy: 0.5571 - val_mae: 0.1192\n",
            "Epoch 17/100\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 1.1651 - accuracy: 0.5077 - mae: 0.1235 - val_loss: 1.0662 - val_accuracy: 0.5724 - val_mae: 0.1199\n",
            "Epoch 18/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1555 - accuracy: 0.5112 - mae: 0.1229 - val_loss: 1.0625 - val_accuracy: 0.5714 - val_mae: 0.1196\n",
            "Epoch 19/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1570 - accuracy: 0.5143 - mae: 0.1233 - val_loss: 1.0605 - val_accuracy: 0.5663 - val_mae: 0.1194\n",
            "Epoch 20/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1538 - accuracy: 0.5123 - mae: 0.1228 - val_loss: 1.0568 - val_accuracy: 0.5684 - val_mae: 0.1197\n",
            "Epoch 21/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1493 - accuracy: 0.5123 - mae: 0.1229 - val_loss: 1.0494 - val_accuracy: 0.5694 - val_mae: 0.1185\n",
            "Epoch 22/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1474 - accuracy: 0.5120 - mae: 0.1226 - val_loss: 1.0490 - val_accuracy: 0.5673 - val_mae: 0.1189\n",
            "Epoch 23/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1395 - accuracy: 0.5212 - mae: 0.1220 - val_loss: 1.0460 - val_accuracy: 0.5643 - val_mae: 0.1182\n",
            "Epoch 24/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1354 - accuracy: 0.5143 - mae: 0.1213 - val_loss: 1.0427 - val_accuracy: 0.5745 - val_mae: 0.1178\n",
            "Epoch 25/100\n",
            "123/123 [==============================] - 1s 6ms/step - loss: 1.1425 - accuracy: 0.5138 - mae: 0.1221 - val_loss: 1.0414 - val_accuracy: 0.5622 - val_mae: 0.1168\n",
            "Epoch 26/100\n",
            "123/123 [==============================] - 1s 6ms/step - loss: 1.1265 - accuracy: 0.5161 - mae: 0.1211 - val_loss: 1.0402 - val_accuracy: 0.5714 - val_mae: 0.1176\n",
            "Epoch 27/100\n",
            "123/123 [==============================] - 1s 6ms/step - loss: 1.1270 - accuracy: 0.5271 - mae: 0.1211 - val_loss: 1.0352 - val_accuracy: 0.5735 - val_mae: 0.1167\n",
            "Epoch 28/100\n",
            "123/123 [==============================] - 1s 7ms/step - loss: 1.1298 - accuracy: 0.5138 - mae: 0.1211 - val_loss: 1.0347 - val_accuracy: 0.5745 - val_mae: 0.1175\n",
            "Epoch 29/100\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 1.1249 - accuracy: 0.5209 - mae: 0.1212 - val_loss: 1.0316 - val_accuracy: 0.5714 - val_mae: 0.1168\n",
            "Epoch 30/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1151 - accuracy: 0.5199 - mae: 0.1201 - val_loss: 1.0323 - val_accuracy: 0.5735 - val_mae: 0.1168\n",
            "Epoch 31/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.1166 - accuracy: 0.5276 - mae: 0.1202 - val_loss: 1.0338 - val_accuracy: 0.5837 - val_mae: 0.1171\n",
            "Epoch 32/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1205 - accuracy: 0.5204 - mae: 0.1210 - val_loss: 1.0343 - val_accuracy: 0.5684 - val_mae: 0.1170\n",
            "Epoch 33/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.1126 - accuracy: 0.5212 - mae: 0.1200 - val_loss: 1.0324 - val_accuracy: 0.5796 - val_mae: 0.1176\n",
            "Epoch 34/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.1197 - accuracy: 0.5184 - mae: 0.1210 - val_loss: 1.0283 - val_accuracy: 0.5735 - val_mae: 0.1171\n",
            "Epoch 35/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.1051 - accuracy: 0.5197 - mae: 0.1201 - val_loss: 1.0271 - val_accuracy: 0.5643 - val_mae: 0.1160\n",
            "Epoch 36/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1066 - accuracy: 0.5268 - mae: 0.1198 - val_loss: 1.0235 - val_accuracy: 0.5694 - val_mae: 0.1159\n",
            "Epoch 37/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0958 - accuracy: 0.5360 - mae: 0.1191 - val_loss: 1.0231 - val_accuracy: 0.5857 - val_mae: 0.1160\n",
            "Epoch 38/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0980 - accuracy: 0.5186 - mae: 0.1195 - val_loss: 1.0261 - val_accuracy: 0.5816 - val_mae: 0.1168\n",
            "Epoch 39/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0994 - accuracy: 0.5350 - mae: 0.1195 - val_loss: 1.0238 - val_accuracy: 0.5827 - val_mae: 0.1165\n",
            "Epoch 40/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0990 - accuracy: 0.5179 - mae: 0.1196 - val_loss: 1.0268 - val_accuracy: 0.5765 - val_mae: 0.1174\n",
            "Epoch 41/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1008 - accuracy: 0.5276 - mae: 0.1195 - val_loss: 1.0281 - val_accuracy: 0.5745 - val_mae: 0.1174\n",
            "Epoch 42/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1064 - accuracy: 0.5337 - mae: 0.1198 - val_loss: 1.0188 - val_accuracy: 0.5776 - val_mae: 0.1155\n",
            "Epoch 43/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0949 - accuracy: 0.5370 - mae: 0.1189 - val_loss: 1.0204 - val_accuracy: 0.5755 - val_mae: 0.1164\n",
            "Epoch 44/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0929 - accuracy: 0.5329 - mae: 0.1190 - val_loss: 1.0241 - val_accuracy: 0.5857 - val_mae: 0.1166\n",
            "Epoch 45/100\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 1.1028 - accuracy: 0.5235 - mae: 0.1193 - val_loss: 1.0242 - val_accuracy: 0.5827 - val_mae: 0.1171\n",
            "Epoch 46/100\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 1.0975 - accuracy: 0.5273 - mae: 0.1195 - val_loss: 1.0238 - val_accuracy: 0.5765 - val_mae: 0.1171\n",
            "Epoch 47/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1012 - accuracy: 0.5345 - mae: 0.1192 - val_loss: 1.0207 - val_accuracy: 0.5816 - val_mae: 0.1166\n",
            "Epoch 48/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.1007 - accuracy: 0.5217 - mae: 0.1197 - val_loss: 1.0146 - val_accuracy: 0.5867 - val_mae: 0.1156\n",
            "Epoch 49/100\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 1.0962 - accuracy: 0.5316 - mae: 0.1191 - val_loss: 1.0156 - val_accuracy: 0.5816 - val_mae: 0.1160\n",
            "Epoch 50/100\n",
            "123/123 [==============================] - 1s 5ms/step - loss: 1.0811 - accuracy: 0.5368 - mae: 0.1184 - val_loss: 1.0141 - val_accuracy: 0.5745 - val_mae: 0.1153\n",
            "Epoch 51/100\n",
            "123/123 [==============================] - 1s 6ms/step - loss: 1.0956 - accuracy: 0.5271 - mae: 0.1190 - val_loss: 1.0159 - val_accuracy: 0.5806 - val_mae: 0.1156\n",
            "Epoch 52/100\n",
            "123/123 [==============================] - 1s 5ms/step - loss: 1.0865 - accuracy: 0.5380 - mae: 0.1186 - val_loss: 1.0204 - val_accuracy: 0.5776 - val_mae: 0.1163\n",
            "Epoch 53/100\n",
            "123/123 [==============================] - 1s 6ms/step - loss: 1.0895 - accuracy: 0.5365 - mae: 0.1189 - val_loss: 1.0151 - val_accuracy: 0.5806 - val_mae: 0.1161\n",
            "Epoch 54/100\n",
            "123/123 [==============================] - 1s 6ms/step - loss: 1.0843 - accuracy: 0.5311 - mae: 0.1188 - val_loss: 1.0133 - val_accuracy: 0.5837 - val_mae: 0.1158\n",
            "Epoch 55/100\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 1.0852 - accuracy: 0.5393 - mae: 0.1185 - val_loss: 1.0150 - val_accuracy: 0.5918 - val_mae: 0.1160\n",
            "Epoch 56/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0790 - accuracy: 0.5362 - mae: 0.1181 - val_loss: 1.0125 - val_accuracy: 0.5888 - val_mae: 0.1154\n",
            "Epoch 57/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.1014 - accuracy: 0.5204 - mae: 0.1188 - val_loss: 1.0155 - val_accuracy: 0.5888 - val_mae: 0.1165\n",
            "Epoch 58/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0758 - accuracy: 0.5383 - mae: 0.1182 - val_loss: 1.0114 - val_accuracy: 0.5806 - val_mae: 0.1150\n",
            "Epoch 59/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0854 - accuracy: 0.5314 - mae: 0.1179 - val_loss: 1.0132 - val_accuracy: 0.5888 - val_mae: 0.1158\n",
            "Epoch 60/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0775 - accuracy: 0.5322 - mae: 0.1180 - val_loss: 1.0140 - val_accuracy: 0.5878 - val_mae: 0.1158\n",
            "Epoch 61/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0710 - accuracy: 0.5429 - mae: 0.1174 - val_loss: 1.0101 - val_accuracy: 0.5837 - val_mae: 0.1150\n",
            "Epoch 62/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0821 - accuracy: 0.5342 - mae: 0.1182 - val_loss: 1.0115 - val_accuracy: 0.5878 - val_mae: 0.1149\n",
            "Epoch 63/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0834 - accuracy: 0.5334 - mae: 0.1183 - val_loss: 1.0091 - val_accuracy: 0.5847 - val_mae: 0.1155\n",
            "Epoch 64/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0783 - accuracy: 0.5263 - mae: 0.1181 - val_loss: 1.0134 - val_accuracy: 0.5939 - val_mae: 0.1159\n",
            "Epoch 65/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0718 - accuracy: 0.5459 - mae: 0.1174 - val_loss: 1.0131 - val_accuracy: 0.5837 - val_mae: 0.1153\n",
            "Epoch 66/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0799 - accuracy: 0.5368 - mae: 0.1181 - val_loss: 1.0095 - val_accuracy: 0.5878 - val_mae: 0.1153\n",
            "Epoch 67/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0753 - accuracy: 0.5324 - mae: 0.1179 - val_loss: 1.0145 - val_accuracy: 0.5867 - val_mae: 0.1157\n",
            "Epoch 68/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0791 - accuracy: 0.5304 - mae: 0.1181 - val_loss: 1.0078 - val_accuracy: 0.5847 - val_mae: 0.1153\n",
            "Epoch 69/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0738 - accuracy: 0.5388 - mae: 0.1177 - val_loss: 1.0061 - val_accuracy: 0.5918 - val_mae: 0.1151\n",
            "Epoch 70/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0774 - accuracy: 0.5370 - mae: 0.1178 - val_loss: 1.0063 - val_accuracy: 0.5857 - val_mae: 0.1148\n",
            "Epoch 71/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0724 - accuracy: 0.5352 - mae: 0.1174 - val_loss: 1.0089 - val_accuracy: 0.5898 - val_mae: 0.1151\n",
            "Epoch 72/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0746 - accuracy: 0.5447 - mae: 0.1176 - val_loss: 1.0092 - val_accuracy: 0.5765 - val_mae: 0.1142\n",
            "Epoch 73/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0745 - accuracy: 0.5314 - mae: 0.1176 - val_loss: 1.0066 - val_accuracy: 0.5898 - val_mae: 0.1147\n",
            "Epoch 74/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0726 - accuracy: 0.5273 - mae: 0.1176 - val_loss: 1.0076 - val_accuracy: 0.5898 - val_mae: 0.1151\n",
            "Epoch 75/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0849 - accuracy: 0.5362 - mae: 0.1182 - val_loss: 1.0094 - val_accuracy: 0.5888 - val_mae: 0.1157\n",
            "Epoch 76/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0613 - accuracy: 0.5365 - mae: 0.1174 - val_loss: 1.0038 - val_accuracy: 0.5908 - val_mae: 0.1139\n",
            "Epoch 77/100\n",
            "123/123 [==============================] - 1s 6ms/step - loss: 1.0792 - accuracy: 0.5375 - mae: 0.1175 - val_loss: 1.0044 - val_accuracy: 0.5776 - val_mae: 0.1146\n",
            "Epoch 78/100\n",
            "123/123 [==============================] - 1s 6ms/step - loss: 1.0788 - accuracy: 0.5393 - mae: 0.1177 - val_loss: 1.0087 - val_accuracy: 0.5908 - val_mae: 0.1154\n",
            "Epoch 79/100\n",
            "123/123 [==============================] - 1s 6ms/step - loss: 1.0789 - accuracy: 0.5329 - mae: 0.1181 - val_loss: 1.0121 - val_accuracy: 0.5847 - val_mae: 0.1158\n",
            "Epoch 80/100\n",
            "123/123 [==============================] - 1s 7ms/step - loss: 1.0590 - accuracy: 0.5467 - mae: 0.1170 - val_loss: 1.0053 - val_accuracy: 0.5898 - val_mae: 0.1145\n",
            "Epoch 81/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0718 - accuracy: 0.5424 - mae: 0.1174 - val_loss: 1.0062 - val_accuracy: 0.5898 - val_mae: 0.1151\n",
            "Epoch 82/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0664 - accuracy: 0.5444 - mae: 0.1172 - val_loss: 1.0044 - val_accuracy: 0.5837 - val_mae: 0.1143\n",
            "Epoch 83/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0594 - accuracy: 0.5419 - mae: 0.1168 - val_loss: 0.9996 - val_accuracy: 0.5918 - val_mae: 0.1134\n",
            "Epoch 84/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0650 - accuracy: 0.5424 - mae: 0.1162 - val_loss: 1.0061 - val_accuracy: 0.5949 - val_mae: 0.1154\n",
            "Epoch 85/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0690 - accuracy: 0.5352 - mae: 0.1173 - val_loss: 1.0053 - val_accuracy: 0.5898 - val_mae: 0.1148\n",
            "Epoch 86/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0655 - accuracy: 0.5413 - mae: 0.1174 - val_loss: 1.0016 - val_accuracy: 0.5949 - val_mae: 0.1144\n",
            "Epoch 87/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0694 - accuracy: 0.5385 - mae: 0.1172 - val_loss: 1.0016 - val_accuracy: 0.5959 - val_mae: 0.1143\n",
            "Epoch 88/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0669 - accuracy: 0.5421 - mae: 0.1170 - val_loss: 0.9997 - val_accuracy: 0.5939 - val_mae: 0.1145\n",
            "Epoch 89/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0553 - accuracy: 0.5439 - mae: 0.1165 - val_loss: 1.0013 - val_accuracy: 0.5908 - val_mae: 0.1148\n",
            "Epoch 90/100\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 1.0659 - accuracy: 0.5368 - mae: 0.1169 - val_loss: 1.0035 - val_accuracy: 0.5918 - val_mae: 0.1151\n",
            "Epoch 91/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0612 - accuracy: 0.5429 - mae: 0.1170 - val_loss: 1.0044 - val_accuracy: 0.5918 - val_mae: 0.1150\n",
            "Epoch 92/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0670 - accuracy: 0.5383 - mae: 0.1172 - val_loss: 1.0027 - val_accuracy: 0.5898 - val_mae: 0.1148\n",
            "Epoch 93/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0604 - accuracy: 0.5360 - mae: 0.1168 - val_loss: 1.0028 - val_accuracy: 0.5959 - val_mae: 0.1149\n",
            "Epoch 94/100\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 1.0559 - accuracy: 0.5429 - mae: 0.1164 - val_loss: 1.0006 - val_accuracy: 0.5969 - val_mae: 0.1144\n",
            "Epoch 95/100\n",
            "123/123 [==============================] - 1s 4ms/step - loss: 1.0544 - accuracy: 0.5457 - mae: 0.1165 - val_loss: 0.9990 - val_accuracy: 0.5949 - val_mae: 0.1142\n",
            "Epoch 96/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0478 - accuracy: 0.5493 - mae: 0.1163 - val_loss: 0.9992 - val_accuracy: 0.5969 - val_mae: 0.1141\n",
            "Epoch 97/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0558 - accuracy: 0.5431 - mae: 0.1164 - val_loss: 0.9966 - val_accuracy: 0.5918 - val_mae: 0.1137\n",
            "Epoch 98/100\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 1.0573 - accuracy: 0.5436 - mae: 0.1165 - val_loss: 1.0001 - val_accuracy: 0.5898 - val_mae: 0.1132\n",
            "Epoch 99/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0601 - accuracy: 0.5393 - mae: 0.1164 - val_loss: 0.9967 - val_accuracy: 0.5908 - val_mae: 0.1137\n",
            "Epoch 100/100\n",
            "123/123 [==============================] - 0s 4ms/step - loss: 1.0622 - accuracy: 0.5408 - mae: 0.1168 - val_loss: 0.9970 - val_accuracy: 0.5980 - val_mae: 0.1139\n",
            "123/123 [==============================] - 0s 2ms/step - loss: 1.0074 - accuracy: 0.5613 - mae: 0.1147\n",
            "31/31 [==============================] - 0s 2ms/step - loss: 0.9970 - accuracy: 0.5980 - mae: 0.1139\n",
            "Train Loss: 1.0074232816696167, Train Accuracy: 0.5612557530403137, Train MAE: 0.11466038972139359\n",
            "Test Loss: 0.9969601035118103, Test Accuracy: 0.5979591608047485, Test MAE: 0.11391482502222061\n"
          ]
        }
      ],
      "source": [
        "# build structure of the model\n",
        "model = kb.Sequential([\n",
        "    kb.layers.Dense(256, activation = 'relu', input_shape=(X_train.shape[1],)),\n",
        "    kb.layers.Dropout(0.5),\n",
        "    kb.layers.Dense(128, activation = 'relu'),\n",
        "    kb.layers.Dropout(0.3),\n",
        "    kb.layers.Dense(64, activation = 'relu'),\n",
        "    kb.layers.Dropout(0.2),\n",
        "    kb.layers.Dense(32, activation = 'relu'),\n",
        "    kb.layers.Dropout(0.1),\n",
        "    kb.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(0.01),\n",
        "              metrics=['accuracy', 'mae'])\n",
        "\n",
        "# one hot encoding to help fix error\n",
        "\n",
        "y_train_encoded = to_categorical(y_train - 1, num_classes=10)\n",
        "y_test_encoded = to_categorical(y_test - 1, num_classes=10)\n",
        "\n",
        "\n",
        "#fit the model\n",
        "model.fit(X_train, y_train_encoded, epochs = 100, validation_data=(X_test, y_test_encoded))\n",
        "\n",
        "# Evaluate the model and getting metrics\n",
        "train_metrics = model.evaluate(X_train, y_train_encoded)\n",
        "test_metrics = model.evaluate(X_test, y_test_encoded)\n",
        "\n",
        "print(f'Train Loss: {train_metrics[0]}, Train Accuracy: {train_metrics[1]}, Train MAE: {train_metrics[2]}')\n",
        "print(f'Test Loss: {test_metrics[0]}, Test Accuracy: {test_metrics[1]}, Test MAE: {test_metrics[2]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9yn88j2NCiq",
        "outputId": "0ae90d66-051d-4567-d653-0bdd857dfacc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Train Accuracy: 0.5336906584992342\n",
            "Logistic Regression Train MAE: 0.5222052067381318\n",
            "Logistic Regression Test Accuracy: 0.5551020408163265\n",
            "Logistic Regression Test MAE: 0.5112244897959184\n"
          ]
        }
      ],
      "source": [
        "# Logistic regression model building\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "train_preds = log_reg.predict(X_train)\n",
        "test_preds = log_reg.predict(X_test)\n",
        "\n",
        "# Calculating accuracy\n",
        "train_acc = accuracy_score(y_train, train_preds)\n",
        "test_acc = accuracy_score(y_test, test_preds)\n",
        "\n",
        "# Calculating MAE\n",
        "train_mae = mean_absolute_error(y_train, train_preds)\n",
        "test_mae = mean_absolute_error(y_test, test_preds)\n",
        "\n",
        "# Output metrics\n",
        "print(f'Logistic Regression Train Accuracy: {train_acc}')\n",
        "print(f'Logistic Regression Train MAE: {train_mae}')\n",
        "\n",
        "print(f'Logistic Regression Test Accuracy: {test_acc}')\n",
        "print(f'Logistic Regression Test MAE: {test_mae}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}